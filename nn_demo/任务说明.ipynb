{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 总述",
   "id": "aa17e7e1e0f2258c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这部分要求构建一个基本的softmax回归算法，以及构建一个简单的两层神经网络。将使用原生Python（使用numpy库），不借助keras实现这些算法\n",
    "\n",
    "在此过程中，将提供一些关于如何实现这些不同函数的指导，但总体而言，细节需要自己实现。 应该尽量使用 numpy 中的线性代数调用：for/while循环通常会使代码运行速度比预期慢得多。"
   ],
   "id": "6c3f1fe62132569f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 一、文件说明\n",
    "\n",
    "```\n",
    "data/\n",
    "    train-images-idx3-ubyte.gz\n",
    "    train-labels-idx1-ubyte.gz\n",
    "    t10k-images-idx3-ubyte.gz\n",
    "    t10k-labels-idx1-ubyte.gz\n",
    "neural_network/\n",
    "    p1.png\n",
    "    p2.png\n",
    "src/\n",
    "    simple_nn.py\n",
    "tests/\n",
    "    test_simple_nn.py\n",
    "```\n",
    "\n",
    "data/ 目录包含这部分作业所需的数据（MNIST 数据集的副本）；\n",
    "\n",
    "neural_network/ 目录包含相关图片\n",
    "\n",
    "src/ 目录包含实现功能所需的源代码；\n",
    "\n",
    "tests/ 目录包含用于测试实现代码是否正确的代码"
   ],
   "id": "ea11476507a99c11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去命令行（cmd/Anaconda Powershell Prompt /其他终端）运行如下指令（激活开发环境一定要最先执行），安装这部分作业依赖的python库：\n",
    "- 激活开发环境：conda activate 环境名（以下用homl3举例）\n",
    "- 安装numdifftools：conda install numdifftools\n",
    "- 安装pytest：conda install pytest\n"
   ],
   "id": "bbec9387c82d28a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 二、具体实现步骤",
   "id": "6ecc53d7a989f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第一题：简单的加法函数，以及使用pytest测试代码",
   "id": "4c898f53cabb194f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了说明这部分的代码+数据，以及pytest使用，将使用一个实现 add函数 的简单示例。\n",
    "\n",
    "第一题要求实现 src/目录里 simple_nn.py内的 add函数（这个简单的函数实际上并没有用到，它只是一个帮助熟悉作业结构的示例）。查看 src/simple_nn.py 文件，将找到 add() 函数的定义"
   ],
   "id": "e9c91dfa3c0912a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 测试代码",
   "id": "919bb56f32b06378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在需要测试一下你的代码是否能正确运行，正确运行才说明实现没问题。\n",
    "\n",
    "在这部分作业中，将使用pytest对代码进行单元测试。在 src/simple_nn.py 文件中 写完 add函数的实现后，去命令行里确保已经激活了homl3环境（conda activate homl3）， 确保homl3环境里安装过了numdifftools和pytest，确定命令行里显示的文件路径在 nn_demo 的目录下（这个目录同时有data/, src/和tests/文件夹），然后执行以下命令：\n",
    "\n",
    "python -m pytest -k \"add\"\n",
    "\n",
    "如果一切正常，你会看到类似这样的图片：\n",
    "![测试add通过](./neural_network/p1.png)\n",
    "\n",
    "想看测试如何进行的，可以去查看tests/test_simple_nn.py文件，python -m pytest -k \"add\"指令刚刚运行的是 文件里的test_add() 函数\n",
    "\n",
    "如果错误地实现了某些内容（例如，将上面的 x + y 更改为 x - y），那么测试将会失败，并且 pytest 将会指示相应的测试失败。\n",
    "\n",
    "比如把x+y，换成x-y后，执行python -m pytest -k \"add\"：\n",
    "![测试add不通过](./neural_network/p2.png)"
   ],
   "id": "8cd29340f4d80aac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "如图所见，将收到一个错误，指示断言失败，然后就可以使用它来返回并调整实现代码。应该能够熟练地阅读和跟踪测试文件，以便更好地理解正确的实现应该如何工作\n",
    "\n",
    "学习正确开发和使用单元测试对于现代软件开发至关重要，希望这个demo可以帮助你了解单元测试在软件开发中的典型用法。\n",
    "\n",
    "当然，这次作业不一定需要编写自己的测试去确保自己实现正确，但应该熟悉如何阅读提供的测试文件，以便了解要实现的函数应该如何运行。但是，也绝对鼓励为自己的实现编写额外的测试。\n",
    "\n",
    "如果习惯通过打印语句调试代码，请注意，pytest 默认会捕获任何输出（隐藏掉测试代码执行的print）。可以通过将 -s 传递给 pytest 来禁用此行为并让测试在所有情况下显示所有输出。"
   ],
   "id": "33304ba8ce802fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第二题：用gzip和struct处理压缩文件和二进制数据，加载MNIST数据",
   "id": "ebec5034752b27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在已经熟悉了测试工具pytest，接下来在 src/simple_nn.py 中需要实现的函数上尝试一下：parse_mnist_data() 函数。\n",
    "\n",
    "这个函数也有文档字符串（docstring），请仔细阅读它们。\n",
    "\n",
    "然后，请访问 https://web.archive.org/web/20220509025752/http://yann.lecun.com/exdb/mnist/ 了解 MNIST 数据的二进制格式。然后编写函数读取此类文件，并根据文档字符串中的规范返回 numpy 数组）。建议使用 Python 中的 struct 模块（以及 gzip 模块，当然还有 numpy 本身）来实现此函数。\n",
    "\n",
    "实现函数后，去命令行运行本地单元测试， 同样确保命令行激活了homl3环境，确保路径在nn_demo目录下（有data/,src/和tests/文件）， 后面的题不再强调\n",
    "\n",
    "python -m pytest -k \"parse_mnist\""
   ],
   "id": "f22b95e3905b669b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MNIST文件格式",
   "id": "ffd0cbd497a2410e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "相关地址 https://web.archive.org/web/20220509025752/http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "1.图像文件 (train-images-idx3-ubyte, t10k-images-idx3-ubyte):\n",
    "\n",
    "- 偏移量 0：魔数（4字节，MSB first），固定为 0x00000803 (2051)\n",
    "    - `0x0000`：保留部分\n",
    "    - `0x08`: 表示数据类型（08 代表无符号字节）\n",
    "    - `0x03`: 表示维度数量（03 代表3维：$n_{samples} \\times \\text{row}_{num} \\times \\text{col}_{num}$）\n",
    "\n",
    "- 偏移量 4：图像数量（4字节）\n",
    "\n",
    "- 偏移量 8：行数（4字节）\n",
    "\n",
    "- 偏移量 12：列数（4字节）\n",
    "\n",
    "- 偏移量 16：开始是图像数据，每个像素是1字节的无符号整数（0-255）\n",
    "\n",
    "2.标签文件 (train-labels-idx1-ubyte, t10k-labels-idx1-ubyte):\n",
    "\n",
    "- 偏移量 0：魔数（4字节，MSB first），固定为 0x00000801 (2049)\n",
    "    - `0x0000`：保留部分\n",
    "    - `0x08`: 表示数据类型（08 代表无符号字节）\n",
    "    - `0x01`: 表示维度数量（01 代表1维：$n_{samples}$）\n",
    "\n",
    "- 偏移量 4：标签数量（4字节）\n",
    "\n",
    "- 偏移量 8：开始是标签数据，每个标签是1字节的无符号整数（0-9）"
   ],
   "id": "73663d4ef3eedac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "魔数（Magic Number）\n",
    "\n",
    "- 定义：\n",
    "\n",
    "魔数是用于标识文件类型和结构的特殊数值，通常位于文件的开头部分。它就像一个数字签名，可以帮助程序快速识别文件格式\n",
    "\n",
    "- 魔数的作用\n",
    "\n",
    "1. **文件类型识别**：快速确定文件是否属于预期的格式\n",
    "\n",
    "2. **版本验证**：确保文件与程序兼容\n",
    "3. **错误检测**：防止程序错误的处理不兼容的文件\n",
    "4. **字节序检测**：帮助确定文件使用的字节序（大端或小端）\n",
    "    - 字节序\n",
    "    > 指在计算机系统中，多字节数据在内存中存储的字节顺序，它决定了字节的排列方式\n",
    "        - 大端序：最高有效字节存储在最低内存地址\n",
    "        - 小端序：最低有效字节存储在最低内存地址\n",
    "> 例子（数字 `0x12345678` 的存储方式）\n",
    "- 大端序\n",
    "```text\n",
    "地址：0x1000 0x1001 0x1002 0x1003\n",
    "数据：0x12   0x34   0x56   0x78\n",
    "```\n",
    "\n",
    "- 小端序\n",
    "```text\n",
    "地址：0x1000 0x1001 0x1002 0x1003\n",
    "数据：0x78   0x56   0x34   0x12\n",
    "```"
   ],
   "id": "f97db3e94ad1f767"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第三题：Softmax损失",
   "id": "1323562d0cba4cfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在 `src/simple_nn.py` 文件的 `softmax_loss()` 函数中实现 softmax（也称为交叉熵）损失。\n",
    "\n",
    "对于一个可以取值 $ y \\in \\{1, \\ldots, k\\} $ 的多类输出，softmax 损失接收一个对数几率向量 $ z \\in \\mathbb{R}^k $ 和真实类别 $ y \\in \\{1, \\ldots, k\\} $ 作为输入，并返回由以下公式定义的损失：\n",
    "\n",
    "$$\n",
    "\\ell_{\\text{softmax}}(z, y) = \\log (\\sum_{i=1}^{k} \\exp z_i) - z_y\n",
    "$$\n",
    "\n",
    "对数几率向量z，可以看成被softmax激活之前的值\n",
    "\n",
    "请注意，如其文档字符串（docstring）所述，`softmax_loss()` 函数接收一个二维的对数几率数组（即，一批不同样本的 $ k $ 维对数几率）加上一个对应的一维真实标签数组，并应返回整批样本的平均 softmax 损失。请注意，为了正确实现此功能，你不应使用任何循环，而是完全使用 numpy 的向量化操作进行计算（为此设定预期，实现代码可以少到一行代码）。"
   ],
   "id": "3cdb52032ac61c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实现完成后，可以去命令行进行单元测试：python -m pytest -k \"softmax_loss\"",
   "id": "413eec903944c28e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第四题：softmax回归小批量梯度下降",
   "id": "a3f4e9cefaaa8dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在这个问题中，你将实现（线性）softmax 回归的小批量梯度下降）\n",
    "\n",
    "考虑一个假设函数，该函数通过以下公式将 $ n $ 维输入转换为 $ k $ 维对数几率：\n",
    "\n",
    "$$\n",
    "h(x) = \\Theta^T x\n",
    "$$\n",
    "\n",
    "其中 $ x \\in \\mathbb{R}^n $ 是输入，$\\Theta \\in \\mathbb{R}^{n \\times k}$ 是模型参数。给定数据集 $\\{(x^{(i)} \\in \\mathbb{R}^n, y^{(i)} \\in \\{1, \\ldots, k\\})\\}$，其中 $ i = 1, \\ldots, m $，softmax 回归相关的优化问题由下式给出：\n",
    "\n",
    "$$\n",
    "\\text{minimize} \\frac{1}{m} \\sum_{i=1}^{m} \\ell_{\\text{softmax}} (\\Theta^T x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "线性 softmax 目标的梯度由下式给出\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell_{\\text{softmax}} (\\Theta^T x, y) = x(z - e_y)^T\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "z = \\frac{\\exp(\\Theta^T x)}{1^T \\exp(\\Theta^T x)} = \\text{normalize}(\\exp(\\Theta^T x))\n",
    "$$\n",
    "\n",
    "（即 $ z $ 只是归一化的 softmax 概率），并且 $ e_y $ 表示 y 分类的独热编码，即一个所有元素为零，只有第 $ y $ 个位置为 1 的向量。\n",
    "\n",
    "也可以用更紧凑的符号来表示，方便代码实现，即，如果让 $ X \\in \\mathbb{R}^{m \\times n} $ 表示某个 $ m $ 个输入的特征矩阵（整个数据集或一个小批量），$ y \\in \\{1, \\ldots, k\\}^m $ 是对应的标签向量，并且 $ \\ell_{\\text{softmax}} $ 表示平均 softmax 损失，那么\n",
    "\n",
    "$$\n",
    "\\nabla_\\Theta \\ell_{\\text{softmax}}(X \\Theta, y) = \\frac{1}{m} X^T (Z - I_y)\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "Z = \\text{normalize}(\\exp(X \\Theta)) \\quad (\\text{按行进行归一化})\n",
    "$$\n",
    "\n",
    "表示对数几率矩阵，而 $ I_y \\in \\mathbb{R}^{m \\times k} $ 表示 $ y $ 中标签的 逐个转成 独热编码，按行连接\n",
    "\n",
    "使用这些梯度，实现 `softmax_regression_epoch()` 函数，该函数使用指定的学习率/步长 $ \\eta $ 和小批量大小 `batch_size` 运行单个轮次（对数据集的一次遍历）。如其文档字符串所述，你的函数应该就地修改 Theta 数组。实现后，请去命令行运行测试。\n",
    "\n",
    "python -m pytest -k \"softmax_regression_epoch\""
   ],
   "id": "67f005120ef5a202"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 用softmax回归训练MNIST",
   "id": "f0e6a75ce062199d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "虽然这不包含在测试中，但既然你已经编写了这段代码，你也可以尝试使用 SGD 训练一个完整的 MNIST 线性分类器。为此，你可以使用 src/simple_nn.py 文件中的 train_softmax() 函数（已经编写好了这个函数，所以无需自行编写，但可以查看一下它的功能）。\n",
    "\n",
    "可以使用以下代码了解它的工作原理。作为参考，如下所示，我的实现在 notebook 上运行时间约为 2 秒，测试集错误率为 7.97%。"
   ],
   "id": "fc5478e35ebb74bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T09:26:08.072184Z",
     "start_time": "2025-08-30T09:26:06.483681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "from simple_nn import train_softmax, parse_mnist\n",
    "\n",
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.2, batch=100)"
   ],
   "id": "2a7bf41f4f1f5211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "|     0 |    0.35134 |   0.10182 |   0.33588 |  0.09400 |\n",
      "|     1 |    0.32142 |   0.09268 |   0.31086 |  0.08730 |\n",
      "|     2 |    0.30802 |   0.08795 |   0.30097 |  0.08550 |\n",
      "|     3 |    0.29987 |   0.08532 |   0.29558 |  0.08370 |\n",
      "|     4 |    0.29415 |   0.08323 |   0.29215 |  0.08230 |\n",
      "|     5 |    0.28981 |   0.08182 |   0.28973 |  0.08090 |\n",
      "|     6 |    0.28633 |   0.08085 |   0.28793 |  0.08080 |\n",
      "|     7 |    0.28345 |   0.07997 |   0.28651 |  0.08040 |\n",
      "|     8 |    0.28100 |   0.07923 |   0.28537 |  0.08010 |\n",
      "|     9 |    0.27887 |   0.07847 |   0.28442 |  0.07970 |\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 第五题：在1个隐藏层的神经网络上进行小批量梯度下降",
   "id": "6e72acd86987b24e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在已经为线性分类器编写了SGD，现在考虑一个简单的两层神经网络的情况。具体来说，对于输入 $ x \\in \\mathbb{R}^n $，考虑一个形式如下的两层神经网络（无偏置项）：\n",
    "\n",
    "$$\n",
    "z = W_2^T ReLU(W_1^T x)\n",
    "$$\n",
    "\n",
    "其中 $ W_1 \\in \\mathbb{R}^{n \\times d} $ 和 $ W_2 \\in \\mathbb{R}^{d \\times k} $ 表示网络的权重（具有 $ d $ 维隐藏单元），而 $ z \\in \\mathbb{R}^k $ 表示网络输出的对数几率。我们再次使用 softmax/交叉熵损失，这意味着我们要解决以下优化问题：\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{W_1, W_2} \\sum_{i=1}^m \\ell_{\\text{softmax}}(W_2^T ReLU(W_1^T x^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "或者，使用矩阵 $ X \\in \\mathbb{R}^{m \\times n} $ 来描述批量形式，这也可以写成：\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y)\n",
    "$$\n",
    "\n",
    "使用链式法则，可以推导出该网络的反向传播更新（为了便于实现，这里提供最终形式）。具体来说，令：\n",
    "\n",
    "$$\n",
    "Z_1 \\in \\mathbb{R}^{m \\times d} = ReLU(XW_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_2 \\in \\mathbb{R}^{m \\times k} = \\text{normalize}(\\exp(Z_1 W_2)) - I_y\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_1 \\in \\mathbb{R}^{m \\times d} = 1\\{Z_1 > 0\\} \\circ (G_2 W_2^T)\n",
    "$$\n",
    "\n",
    "其中 $ 1\\{Z_1 > 0\\} $ 是一个二进制矩阵，其条目根据 $ Z_1 $ 中的每个项是否严格为正而等于零或一，而 $\\circ$ 表示逐元素乘法。那么目标的梯度由下式给出：\n",
    "\n",
    "$$\n",
    "\\nabla_{W_1} \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y) = \\frac{1}{m} X^T G_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{W_2} \\ell_{\\text{softmax}}(ReLU(XW_1)W_2, y) = \\frac{1}{m} Z_1^T G_2\n",
    "$$\n",
    "\n",
    "**注意：** 如果这些精确方程的细节对你来说有点神秘，不必太担心。这些只是两层ReLU网络的标准反向传播方程：$ Z_1 $ 项只是计算\"前向\"传播，而 $ G_2 $ 和 $ G_1 $ 项表示反向传播。但是更新的精确形式可能会因你使用的神经网络符号、制定损失函数的具体方式、是否之前以矩阵形式推导过这些等因素而有所不同。（毕竟，在某种程度上，深度学习系统（比如tensorflow）的整个重点是我们不需要费心进行这些手动计算）。\n"
   ],
   "id": "f6a35e81834a15a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "使用这些梯度，现在在 src/simple_nn.py 文件中编写 nn_epoch() 函数。与上一个问题一样，你的解决方案应该修改 W1 和 W2 数组。实现该函数后，运行以下测试。请务必使用上述表达式所示的矩阵运算来实现该函数：这比尝试使用循环更快、更高效（并且所需的代码也更少）。\n",
    "\n",
    "实现完成后，去命令运行单元测试：python -m pytest -k \"nn_epoch\""
   ],
   "id": "53c19e9d5f2d9c32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 训练神经网络",
   "id": "ff593cbba822cafb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T09:56:00.519538Z",
     "start_time": "2025-08-30T09:55:22.245695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "import importlib\n",
    "import simple_nn\n",
    "importlib.reload(simple_nn) # 重新载入simple_nn， 防止刚才的训练代码产生了缓存，影响了simple_nn\n",
    "\n",
    "sys.path.append(\"src/\")\n",
    "from simple_nn import train_nn, parse_mnist\n",
    "\n",
    "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "                         \"data/train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
    "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
    "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
   ],
   "id": "3f41ff4fe2309f37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "|     0 |    0.15324 |   0.04697 |   0.16305 |  0.04920 |\n",
      "|     1 |    0.09830 |   0.02912 |   0.11569 |  0.03620 |\n",
      "|     2 |    0.07491 |   0.02217 |   0.09840 |  0.03240 |\n",
      "|     3 |    0.05956 |   0.01710 |   0.08756 |  0.02820 |\n",
      "|     4 |    0.04898 |   0.01365 |   0.08178 |  0.02610 |\n",
      "|     5 |    0.04061 |   0.01097 |   0.07723 |  0.02420 |\n",
      "|     6 |    0.03476 |   0.00913 |   0.07429 |  0.02380 |\n",
      "|     7 |    0.03017 |   0.00762 |   0.07241 |  0.02260 |\n",
      "|     8 |    0.02661 |   0.00637 |   0.07115 |  0.02220 |\n",
      "|     9 |    0.02362 |   0.00555 |   0.07003 |  0.02160 |\n",
      "|    10 |    0.02109 |   0.00462 |   0.06911 |  0.02130 |\n",
      "|    11 |    0.01910 |   0.00395 |   0.06822 |  0.02060 |\n",
      "|    12 |    0.01718 |   0.00335 |   0.06755 |  0.02120 |\n",
      "|    13 |    0.01555 |   0.00292 |   0.06696 |  0.02130 |\n",
      "|    14 |    0.01413 |   0.00235 |   0.06656 |  0.02100 |\n",
      "|    15 |    0.01295 |   0.00207 |   0.06609 |  0.02080 |\n",
      "|    16 |    0.01188 |   0.00175 |   0.06586 |  0.02010 |\n",
      "|    17 |    0.01083 |   0.00145 |   0.06526 |  0.01960 |\n",
      "|    18 |    0.00989 |   0.00117 |   0.06496 |  0.01890 |\n",
      "|    19 |    0.00922 |   0.00103 |   0.06482 |  0.01930 |\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "作为参考，我的实现花了30多秒训练，最终在mnist的测试集达到了1.93%的错误率，只用了大概20多行代码",
   "id": "24c53df020c227d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
