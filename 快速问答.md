# # 机器学习

## # 分类相关

### 一、基本问题

- **1、为什么分类要编码？ 分类的独热编码是什么？ 它解决了普通分类编码 （分类标为0，1，2，...）的什么问题？**

​	 因为数据分类的类别很多，计算机训练数据时又需要数值型的数据，所以需要分类编码；

​	独热编码是数据的每一个分类只占一个编码，且编码数值为1，其他为0

​	主要解决了普通分类编码的不一致性问题，如果存在1,2...，计算机会误认为这些类别是有序的



- **2、pd.get_dummies也可以把分类转成独热编码，它和sklearn的OneHotEncoder的核心区别是什么？**

​		pandas的get_dummies不能处理分类列表中分类不存在的数据



- **3、假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？**

​		两个逻辑回归分类器较好，因为逻辑回归支持多标签分类。softmax仅适用于单标签多分类问题



### 二、混淆矩阵

- **1、简述下混淆矩阵是什么？**

​		混淆矩阵是机器学习中表示两个类别是否混淆的标志，它的每一行表示实际值，每一列表示预测值



- **2、简述下各个性能指标的意思： 准确率，召回率，F1分数，假阳性，PR曲线，ROC曲线，AUC分数**

​	**:one:准确率、召回率和F1分数**定义如下
$$
准确率 & \text{precision} = \frac{TP}{TP + FP} \\ \\
召回率 & \text{recall} = \frac{TP}{TP + FN} \\ \\
F1分数 & F_1 = \ \frac{2}{\frac{1}{\text{precision}} + \frac{1}{\text{recall}}} = \frac{2*TP}{2*TP + FP + FN}
$$
其中：TP代表真阳性（实际值为阳性，预测也为阳性），FP代表假阳性（实际值为阳性，预测为阴性），FN为假阴性率（实际值为阴性，预测为阳性），还有未提到的TN，代表真阴性（实际值为阴性，预测也为阴性）



**:two:PR曲线**：精确度-召回率曲线，显示了不同阈值下精确率和召回率之间的权衡，曲线下面积越大，表示召回率和精确率都越高。高准确率指的是返回结果中假阳性较少，而高召回率是指相关结果中假阴性较少，两者得分均较高，表明分类器返回的结果准确。**



**:three:ROC曲线与AUC分数**

ROC曲线的横轴为假阳性率FPR，纵轴为真阳性率（即召回率），定义如下
$$
假阳性率 & FPR = \frac{FP}{FP+TN} \\ \\
真阳性率 & TPR = \frac{TP}{TP+FN}
$$
AUC曲线为ROC曲线下方的面积，AUC面积越接近1模型预测效果越好





- **3、简述下准确率-召回率权衡**

​		准确率和召回率一般是此消彼长的关系，准确率的提高一般要通过提高分类阈值来实现，而提高召回率要通过降低分类阈值来实现，所以要进行准确率-召回率的权衡



- **4、如何用二元分类器 去解决多元分类的问题**

​	使用多个二元分类器组合成一个多元分类器



- **5、什么是 多标签-多分类问题？**

​	它允许每个样本同时属于多个类别标签，不在局限于一个样本一个类别标签







## # 回归相关

- **1、假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？**

​		发生了模型过拟合的现象，使模型的泛化性能很差。

​		解决此问题的三种方法为：减少多项式的项数，增加训练集样本的数量，使用正则化项



- **2、假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？**

​		模型存在高偏差，减小正则化参数



- **3、为什么要使用：**

  - **a.岭回归而不是简单的线性回归（即没有任何正则化）？**

    ​    防止模型过拟合，正则化项惩罚权重大的特征

  - **b.Lasso而不是岭回归？**

    ​	Lasso的L1正则化能产生稀疏解（部分系数 = 0），适合特征选择

  - **c.弹性网络而不是Lasso回归？**

​					弹性网络（L1 + L2）结合了两者的优点，避免了 Lasso 在强相关性特征下的不稳定选择

​		



## # 模型性能评估相关

- **1、什么是k折交叉验证？  验证是在评估什么？**

​		将原始的数据集均分为k个子集，之后依次选择第 `i` $(i = 1,2,3， ...k)$个子集作为验证集，并将剩下的`k-1`个作为训练集；

​		此方法用于评估模型的泛化能力



- **2、精确率是什么，评估分类的性能为什么不能只用精确率？**

​		分类的精确率是混淆矩阵中的 TP / (TP + FP)，当阳性样本数量很少时（极端情况下1个），那么精确率可以达到100%，这不具有代表性





## # 梯度下降相关

- **1、如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？**

​		答案是否；因为梯度下降算法的超参数还有学习率，正则化强度等，就单拿学习率来说，如果学习率调的过大，则损失函数不会收敛到一个最小值，返回可能会使损失函数的值发散，若迭代次数足够多，甚至可以发散到无穷大



- **2、假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，那么可能是什么情况？你该如何解决？**

​		可能是学习率过大或者过拟合的原因，降低学习率或者添加正则项

​	

- **3、当验证误差上升时立即停止小批量梯度下降是个好主意吗？**

​		不是，在小批量梯度下降的情况下，可能用于梯度更新的那一部分训练集样本恰好是噪声数据



- **4、训练逻辑回归模型时，梯度下降可能会卡在局部最小值中吗？**

​		不会，logistic回归的交叉熵损失函数为凸函数，只有一个全局最小值



- **5、哪种梯度下降算法将最快地到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛**

​		SGD（随机梯度下降）会最快到达最优解；批量梯度下降实际上会收敛





## # SVM相关

- **1、支持向量机的基本思想是什么？**

​		寻找一个最优的决策超平面，使得分类数据的间隔尽可能大



- **2、什么是支持向量？**

​		支持向量是SVM中决定决策超平面的关键训练样本。

​		在SVC中，支持向量为位于间隔边界或误分类的样本。而SVR中，支持向量是位于决策边界带之外的样本



- **3、在使用 SVM 时，缩放输入值为什么很重要？**

​		因为SVM对特征的尺度敏感，默认的RBF核就依赖于样本之间的距离，如果未缩放到同一范围，则数值大的特征占主导



- **4、SVM 分类器在对实例进行分类时能输出置信度分数吗？概率呢？**

​		可以通过`decision_function()`输出置信度分数；

​		概率一般不会输出，可以显式的设置`probabilty = True`来对概率进行估计，但是这些概率估计是使用昂贵的五折交叉验证计算的



- **5、你如何在 LinearSVC、SVC 和 SGDClassifier 之间进行选择？**

​		LinearSVC 和 SVC 都适合中小型数据集，而LinearSVC仅适合线性可分的分类数据，SVC可以对非线性可分的数据进行分类

​		SGDClassifier 适合大规模的数据集，灵活度高



- **6、假设你已经使用 RBF 核训练了一个 SVM 分类器，但它似乎欠拟合训练集。你应该增大还是减小 γ（gamma）？C 呢？**

​		增大 $\gamma$，使核函数更敏感，决策边界更复杂，可以是模型更贴合训练数据

​		增大惩罚因子 C， 减少误分类的情况



- **7、ε 不敏感模型是什么意思？**

​		预测值与真实值的误差在 ε 以内时不计算损失，$\epsilon$越大，模型越简单



- **8、使用核技巧有什么意义？**

​		解决决策边界非线性的问题，由于核函数的可以将特征映射到高维空间中，因此在低维空间中线性不可分的数据，可能在高维空间中变的线性可分





## # 决策树相关

- **1、如果训练集有100万个实例，训练决策树（无约束）大致的深度是多少？**

$$
\text{Depth} = \log_2(10^6)
$$



- **2、通常来说，子节点的基尼杂质是高于还是低于其父节点？是通常更高/更低？还是永远更高/更低？**

​		通常低于父节点，是通常，不是永远



- **3、如果决策树过拟合训练集，减少max_depth是否为一个好主意？**

​		是的，减少决策树的最大深度可以减缓过拟合



- **4、如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好主意？**

​		在决策树算法下， 输入特征的尺度大小并不会影响决策树的预测效果，因为决策树是基于特征的阈值去分裂数据的。

​		解决欠拟合的方法可以是：增大决策树的最大深度，或减少每个样本的最小的叶子数量（`min_samples_leaf`）



- **5、如果在给定的训练集上训练决策树需要一个小时，那么如果将特征数量变为两倍，训练大约需要多少时间？如果在包含100万个实例的训练集上训练决策树需要一个小时，那么在包含1000万个实例的训练集上训练决策树，大概需要多长时间？**

​		对于训练集 $X$ 是 $m$ 个样本，$n$ 个特征来说，决策树的CART算法的时间复杂度为$O(nmlog_2m)$

​		假设原算法的时间为 $nmlog_2m$,  则特征数量增加两倍的情况下，时间大约增加两倍。样本数量增加十倍的情况下，时间变为了$n*10mlog_2(10m)$，其中m为$10^6$，则现在的比原来的就是
$$
\frac{n * 10mlog_2(10^7)}{n * mlog_2(10^6)} = 10 \times \frac{log_2(10^7)}{log_2(10^6)} \approx 11.67 倍
$$





## # 集成相关（Ensemble）

- **1、如果已经在完全相同的训练集上训练了5个不同的模型，并且它们都达到了95%的准确率，是否还有机会获得更好的效果？如果可以，该怎么做？ 如果不行，为什么？**

​		可以达到更好的效果，因为这5个模型可能是同类型的模型，对训练集可能会犯相同的错误。可以对这5个模型进行集成，通过投票分类器或堆叠法等集成方法，若集成效果不明显，则可使用不同类型的模型进行组合，或者使用搜索不同的超参数来达到最好的效果



- **2、硬投票分类器和软投票分类器有什么区别？**

​		硬投票分类器的工作原理为：统计每个分类器预测的类别**标签**，选择票数最多的类别作为最终预测。

​		软投票分类器的工作原理为：统计每个分类器预测的类别**概率**，对所有概率求平均，然后选择平均概率最高的类别作为最终预测



- **3、是否可以通过在多个服务器上并行来加速bagging集成的训练？pasting集成呢？提升集成呢？随机森林或堆叠集成呢？**

​		Bagging、Pasting和随机森林完全可并行，boosting集成不可并行，因为是顺序依赖的。堆叠集成部分可以并行



- **4、包外评估的好处是什么？**

​		包外评估无需单独划分验证集，无需交叉验证，它为有放回（Bagging）类算法提供了一个内置的验证集



- **5、是什么让极端随机树比一般随机森林更加随机？这部分增加的随机性有什么用？极端随机树比一般随机森林快还是慢？**

​		随机森林在构建每个树的节点时，会随机选择一个特征子集，然后从这个子集中寻找一个**最优分裂点**（基于基尼指数和信息增益）。而极端随机树会对在子集中**随机**生成一个分裂的阈值，然后从这些随机分裂中选择最好的那一个作为节点分裂规则，因为其更随机性，所以极端随机树一般比随机森林快。

​		增加随机性可以降低方差，减少过拟合风险，降低计算成本



- **6、如果AdaBoost集成对训练数据欠拟合，应该调整哪些超参数？怎么调整？**

​		增加基础估计器（estimator）的复杂度，增加估计器的数量，减少学习率



- **7、如果梯度提升集成对训练集过拟合，应该提升还是降低学习率**

​		降低学习率






## # 杂项

- **1、如果你的训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？**

​		随机梯度下降和小批量梯度下降



- **2、如果你的训练集里特征的数值大小迥异，那么哪些算法可能会受到影响？受影响程度如何？你应该怎么做？**

​		梯度下降类算法会受到影响，例如线性回归、逻辑回归等，其损失值下降的时候会走 "之" 字型路线，需要很大的迭代次数才能收敛到最小值；还有例如KNN/SVM这种相似度敏感的算法。

​		 解决办法：样本特征标准化，必要的时候可以增加正则化项





# # 深度学习

## # 多层感知机相关

- **1、为什么激活函数是训练一个多层感知机（MLP）的关键要素**

​		如果没有激活函数，则多层感知机就等价于一个线性变换，无法学习到非线性的模式



- **2、列举三种常用的激活函数，说明一下它们的大概形状**

​		sigmoid（乙状函数），Relu（$x \ge 0$），tanh（双曲正切函数，S型）



- **3、列出可以在基本MLP（不考虑其他神经网络架构）中进行调整的所有超参数？如果MLP过拟合训练数据，如何调整这些超参数来解决该问题？**

​		MLP中可调整的超参数：神经元数量，隐藏层数量，激活函数，梯度下降的学习率，迭代次数，优化器，每个批次的样本数量大小

​		如果过拟合：可减少神经元数量、隐藏层的数量、迭代次数、学习率。另外还可以增加其他的训练数据



- **4、假设有一个MLP，该MLP由一个输入层，10个特征，随后是一个包含50个神经元的隐藏层，最后是3个神经元组成的输出层。所有人工神经元都使用ReLU激活函数。**

  **a. 输入矩阵X的形状是什么？** $(m, 10)$，其中m是每个批次的样本数量

  **b. 隐藏层的权重W_hidden及其偏置b_hidden的形状分别是什么？** $\mathbf{w}_h.shape = (10, 50), b_h.shape = (50, )$

  **c. 输出层的权重W_output及其偏置b_output的形状是什么?** $\mathbf{w}_o.shape = (50, 3), b_o.shape = (3, )$

  **d. 网络输出矩阵Y的形状是什么?** $(m, 3)$，其中m是每个批次的样本数量

  **e. 写出输出矩阵Y的计算公式，满足Y是W_hidden, b_hidden, W_output, b_output的函数**

  ​	假设激活函数为：
  $$
  \sigma(x) = \text{Relu}(x) = \max(0, x)
  $$
  ​	则：

$$
\mathbf{y}_{hidden} = \sigma(\mathbf{W_hidden^T} X + b_{hidden}) \\
\\
Y = \mathbf{W_output^T} \mathbf{y}_{hidden} + b_{output}
$$

​			对于公式的第二步，可以对其进行激活或不激活，关键在于此网络应用的是回归问题还是分类问题，分类需激活，回归无需激活







## # 神经网络相关

- **1、反向传播的算法解决什么问题，如何工作的**

​		反向传播用于高效计算梯度。工作原理为：神经网络先通过前向传播获取每一层的输出值和激活后的输出值，之后从输出层开始，计算损失函数对权重和偏置的梯度，最后使用梯度下降更新权重和偏置



- **2、如果要将电子邮件分类为垃圾邮件或正常邮件，需要在输出层中有多少个神经元？应该在输出层中使用什么激活函数？相反，如果想解决MNIST图片分类问题，则在输出层中需要有多少个神经元，应该使用哪种激活函数？如何使神经网络预测 回归话题里提到的房价？**

​		:one:对于二分类问题（电子邮件分类为垃圾邮件和正常邮件），输出层应该有2个神经元，使用sigmoid激活函数

​		:two:对于minst多分类问题，输出层需要有K个神经元，K为类别标签的数量，优先使用softmax激活函数

​		:three:对于回归问题，神经网络可以在输出层指定一个神经元，且不使用激活函数



- **3、Glorot初始化和He初始化为了解决什么问题**

​		解决了梯度消失和梯度爆炸问题，在深度神经网络中，如果权重的初始值设置不当，经过多层网络的连续乘法（链式法则），每层的输出的梯度值会急剧增大或减少，从而导致梯度消失或爆炸。



​		初始化权重的方法有`Glorot`初始化、`He`初始化 和 `LeCun`初始化，其权重的分布一般为正态分布和均匀分布

- `Glorot`初始化
  - 均匀分布：服从 $[-\sqrt{6 / (fan_{in} + fan_{out})},\; \sqrt{6 / (fan_{in} + fan_{out})} ]$ 的均匀分布
  - 正态分布：均值为0， 方差为$2/ (fan_{in} + fan_{out})$
- `He`初始化
  - 均匀分布：服从$[-\sqrt{6/fan_{in}}, \;\sqrt{6/fan_{in}}]$的均匀分布
  - 正态分布：均值为0，方差为 $2/fan_{in}$
- `LeCun` 初始化
  - 均匀分布：服从 $[-\sqrt{3/fan_{in}}, \; \sqrt{3/fan_{in}}]$ 的均匀分布
  - 正态分布：均值为0， 方差为 $1/ fan_{in}$

其中：$fan_{in}$ 和 $fan_{out}$ 为每层权重的输入和输出，也称为**扇入和扇出**



另外，对于不同的激活函数，需使用不同的初始化策略，具体使用方法如下：

| 初始化方法   |              激活函数               | 方差 $\sigma^2$ |
| :----------- | :---------------------------------: | :-------------: |
| Glorot初始化 |      None/tanh/sigmoid/softmax      |  $1/fan_{avg}$  |
| He初始化     | ReLU/Leaky ReLU/ELU/GELU/Swish/Mish |  $2/fan_{in}$   |
| Lecun初始化  |                SELU                 |  $1/fan_{in}$   |

其中，$fan_{avg} = (fan_{in} + fan_{out}) / 2$，另外，第三列的方差 $\sigma^2$ 有两个含义，第一个为相对应初始化的正态分布的方差，第二个为相应初始化中均匀分布的范围，即**初始化的权重服从$[-\sqrt{3 \sigma^2}, \; \sqrt{3 \sigma^2}]$的均匀分布**，可以验证，和上述公式的一致性



​		在Keras中，实现相应的初始化只需**传递`kernel_initializer`关键字**即可，例如，`kernel_initializer='he_uniform'`代表He初始化的均匀分布，`kernel_initializer='he_normal'`代表He初始化的正态分布，其他的初始化方法以此类推



- **4、是否可以将所有权重初始化为相同的值（只要该值是使用He初始化随机选择的）？将偏置项初始化为0可以么？**

​		不能将权重初始化为相同的值，因为将所有权重初始化为相同的值等价于这层中的所有神经元被当作一个神经元来看待，这样深度神经网络就会退化，从而达不到很好的效果。

​		可以将偏置项初始化为0，偏置的梯度不像权重的梯度那样依赖于输入值，它只来自于之后层的梯度，即便初始为0，反向传播时，也会更新其值



- **5、列举三种能产生稀疏模型的方法**

​	L1正则化、早停法和剪枝等方法



- **6、dropout会减慢训练速度吗？它会减慢推理（即对新实例进行预测）速度吗？MC dropout呢？**

​		`dropout` 会减慢训练速度，因为，在每次迭代中，dropout会随机丢弃一部分神经元，这意味着每次更新时只是更新了神经网络的一个子集，最终导致损失函数的收敛变慢，因此总的时间会增加。

​		`dropout`不会减慢推理速度，因为在预测阶段，不使用 `dropout`，并且，为了补偿训练时因丢弃神经元而造成的期望输出值的降低，通常会将训练好的权重乘以保留概率（`1-p`），或者训练时对激活值进行缩放

​		`MC dropout` 会减慢训练速度，原因和dropout 减慢训练速度一样。同时，它也会减慢推理（预测）的速度，在推理时，它不关闭`dropout`，而是对同一个输入样本进行多次的前向传播，每次都会随机丢弃不同的神经元，然后对这多次的预测结果取平均和计算方差



### # 激活函数相关

- **1、神经网络的激活函数有什么？并阐述在什么情况下使用它们**

| 激活函数       | 公式                                                         | 适用场景                   |
| :------------- | :----------------------------------------------------------- | :------------------------- |
| **Sigmoid**    | $f(z) = \frac{1}{1 + e⁻ᶻ}$                                   | **二分类输出层**           |
| **Tanh**       | $f(z) = tanh(z)$                                             | **RNN、LSTM的隐藏层**      |
| **Softmax**    | $σ(z_i) = e^{z_i} / \sum_j e^{z_j}$                          | **多分类问题的输出层**     |
| **ReLU**       | $f(z) = max(0, z)$                                           | **默认的隐藏层激活函数**   |
| **Leaky ReLU** | $f(z) = max(\alpha z, z)$                                    | ReLU的替代品，效果可能更好 |
| **ELU/SELU**   | $$f(z) = \begin{cases} \alpha (\exp(z) - 1), & z< 0 \\ z, & z>0\end{cases}$$ | ReLU的平滑变体             |
| **GELU**       | $f(z) = z \Phi(z)$ ，$\Phi(z) = P\{Z<=z\}$                   | 适用于复杂的任务           |
| **Swish**      | $f(z) = z \sigma(\beta z)$, $\sigma(·)$为sigmoid函数         | 视情况而定                 |
| **Mish**       | $f(z) = z \tanh(softplus(z))$，$softplus(z) = \log(1+\exp(z))$ | 视情况而定                 |
| **线性**       | `f(z) = z`                                                   | **回归问题输出层**         |





### # 优化器相关

- **1、如果在使用SGD优化器时将momentum超参数设置得太接近1（例如0.99999)会发生什么情况**

​		收敛速度会变慢，还可能在最优点附近持续振荡，无法稳定收敛，因为过分依赖于之前的梯度，导致每一步的权重更新都会比较迟缓



### # TensorFlow/Keras相关

- **1、TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？**

​		不可以简单替代。

​		① 两者的执行模式不同，

​			NumPy是**立刻执行**，而TensorFlow是**默认图执行**，即它会将代码转成对应的静态计算图之后再执行里面的过程。

​		② 两者的硬件加速不同

​			NumPy主要用于CPU加速，而TensorFlow原生支持CPU、GPU和TPU

​		③ **自动微分的机制**

​			NumPy不支持自动微分，而TensorFlow支持**自动微分， 这是训练神经网络的基础**



- **2、使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？**

​		会得到相同结果，但底层细节会有不同，`tf.range(10)`使用的是TensorFlow的原生操作，直接生成了一个TensorFlow的张量，在随后生成计算图时可能会更高效。而`tf.constant(np.arange(10))`是先生成一个NumPy数组，之后再将此数组转为TensorFlow的张量类型



- **3、可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？**

​		:a:**编写函数**：当你的损失函数非常简单时，即仅仅是对`y_true`和`y_pred`做一个确定的数学运算时，编写函数最为方便

​		:b:**继承Loss类：**当你需要有可配置参数时（例如损失函数的正则化项），或者有复杂的计算逻辑时，继承Loss类



- **4、可以直接在函数中自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？**

​		一般不使用简单函数来自定义指标，因为指标需要累积状态。所以几乎**总是**继承`Metric`类，这样可以逐批次的累积指标



- **5、什么时候应该自定义层而不是自定义模型？**

​		自定义层的目的是创建可重用的基本计算单元，而自定义模型的目的是定义整个网络的结构和组织方式



- **6、有哪些示例需要编写自定义训练循环？**

​		1.对梯度进行自定义操作，例如，梯度裁剪

​		2.需要更详细的日志记录和调试功能



- **7、自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？**

​		可以包含任意Python代码，但是强烈建议将其转换为TensorFlow函数以获得最佳性能



- **8、如果要将函数转换为 TF 函数，应避免哪些主要模式？**

​		①**避免调用外部库**

​			如果调用任何的外部库，包括NumPy基础标准库，此调用将在跟踪过程中执行，它不会成为图的一部分

​		②**随机数生成必须使用TensorFlow提供的方法**

​		③**避免副作用**

​			如果是非TensorFlow的方法是有副作用的，比如`print()`，它只会在第一次TensorFlow构建图的时候执行，即类似`print()`的语句只执行一次，之后不会执行

​		④**慎用Python函数**

​			因为TensorFlow的计算图中只包含TensorFlow的操作，运行Python函数需要先转成TensorFlow的操作，这样会降低模型的性能

​		⑤**Python函数要符合规则**

​			函数和被调用的函数都必须用`@tf.function`修饰

​		⑥**变量必须只创建一次**

​			如果在tf函数内部创建了变量，例如数据集或张量列表，必须确保只创建一次，否则会引发异常。你可以在`__init__`或`build()`方法中创建变了，更新时使用`assign()`，而不是重新赋值

​		⑦**Python源码必须可用才可以用于TensorFlow**

​		⑧**避免使用Python原生循环遍历DataSet**

​			TensorFlow的计算图会捕获张量级别的循环，例如`tf.range()`，而不会捕获Python原生的`for`循环

​		⑨**Python函数中应该尽量实现向量化操作，而不是使用循环**



- **9、何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？**

​		当所需的模型不是简单的堆叠，而是具有循环，宽深等复杂的结构时需创建Keras的动态模型。可以使用函数时API和子类化模型的方式来创建对应的动态模型。根据具体的场景需求，有的顺序的API实现的神经网络模型依旧表现良好。











